# ğŸŒ Air Quality Analysis Project

This project analyzes global air quality data by combining real-time sensor feeds, historical pollution metrics, and public health data. It provides a modular, automated pipeline to collect, clean, store, and visualize data across multiple cities using PostgreSQL, Python, and Power BI.

---

## ğŸš€ Features

- ğŸ“¡ Ingests real-time air quality data from the WAQI API
- ğŸ—ƒ Loads and cleans historical datasets (CSV) and burden of disease data (Excel)
- ğŸ§  Calculates AQI scores using EPA's PM2.5 breakpoints
- ğŸ¥ Merges air quality and health impact datasets
- ğŸ§¼ Deduplicates and standardizes data in PostgreSQL
- ğŸ“Š Ready for Power BI dashboards via clean exports
- â˜ï¸ Hosted on Render with automation via GitHub Actions (runs every 6 hours)

---

## ğŸ“ Folder Structure

```
air_quality_project/
â”œâ”€â”€ config/                     # DB credentials & API token (gitignored)
â”‚   â””â”€â”€ db_config.py
â”‚
â”œâ”€â”€ data/                       # Input datasets 
â”‚   â”œâ”€â”€ Air Quality Datasets/
â”‚   â”œâ”€â”€ Burden Datasets/
â”‚   â””â”€â”€ Cleaned Burden Datasets/
â”‚
â”œâ”€â”€ data_pipeline/              # Core ETL pipeline
â”‚   â”œâ”€â”€ cleaning/
â”‚   â”‚   â””â”€â”€ remove_duplicates.py
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ fetch_waqi.py
â”‚   â”‚   â”œâ”€â”€ import_burden_data.py
â”‚   â”‚   â””â”€â”€ import_historical_air_quality.py
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â””â”€â”€ clean_export_data.py
â”‚   â”œâ”€â”€ transformation/
â”‚   â”‚   â”œâ”€â”€ merge_and_calculate_city_aqi.py
â”‚   â”‚   â”œâ”€â”€ merge_burden_with_aqi.py
â”‚   â”‚   â”œâ”€â”€ merge_public_sources.py
â”‚   â”‚   â””â”€â”€ preprocess_burden_excel.py
â”‚   â”œâ”€â”€ insert_to_db.py
â”‚   â””â”€â”€ run_daily.py
â”‚
â”œâ”€â”€ sql/                        # SQL schema and table setup
â”œâ”€â”€ powerbi/                    # Excel exports and PBIX files
â”œâ”€â”€ logs/                       # Logging outputs
â”œâ”€â”€ main.py                     # Runs the full pipeline
â”œâ”€â”€ .github/workflows/          # GitHub Actions pipeline
â””â”€â”€ requirements.txt
```

---

## ğŸ›  Setup

1. Clone the repo and create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # or venv\Scripts\activate on Windows
   pip install -r requirements.txt
   ```

2. Add your database credentials in `config/db_config.py`:
```python
DB_CONFIG = {
    'host': 'your-host',
    'dbname': 'your-db',
    'user': 'your-user',
    'password': 'your-password',
    'port': 5432
}
```

3. Run the pipeline:
   ```bash
   python main.py
   ```

---

## ğŸ” GitHub Actions

- `run_pipeline.yml`: runs `main.py` every 6 hours
- Uses `DB_CONFIG` secret for credentials

---

## ğŸ“¦ Requirements

All dependencies are listed in `requirements.txt`.

---

## ğŸ“Š Output

Final cleaned table: `final_city_burden_merged` â†’ used in Power BI.

---

## ğŸ”’ Notes

Sensitive files like `.env`, `db_config.py`, and raw data are excluded via `.gitignore`.
